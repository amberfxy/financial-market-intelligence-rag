{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Market Intelligence RAG System\n",
    "## CS6120 Final Project - Colab Version\n",
    "\n",
    "**Team Members:** Soonbee Hwang & Xinyuan Fan (Amber)\n",
    "\n",
    "This notebook runs the complete RAG system in Google Colab.\n",
    "\n",
    "**Note:** Enable GPU runtime (Runtime → Change runtime type → GPU) for faster processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"⚠️ No GPU detected! Consider enabling GPU runtime for better performance.\")\n",
    "    print(\"   Go to: Runtime → Change runtime type → GPU\")\n",
    "\n",
    "# Install all required packages\n",
    "!pip install -q torch transformers sentence-transformers faiss-cpu numpy pandas beautifulsoup4 lxml tiktoken streamlit pyngrok\n",
    "\n",
    "# Install llama-cpp-python (supports both CPU and GPU)\n",
    "# For GPU support, we'll use the default installation which auto-detects CUDA\n",
    "!pip install -q llama-cpp-python\n",
    "\n",
    "# Install Kaggle API\n",
    "!pip install -q kaggle\n",
    "\n",
    "print(\"✅ All dependencies installed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download Dataset\n",
    "\n",
    "### Using Kaggle API Token\n",
    "\n",
    "1. Get your API token from https://www.kaggle.com/settings\n",
    "2. Click \"Create New Token\" and copy the token (format: `KGAT_xxxxx`)\n",
    "3. Update the code below with your token and username\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using API Token (No file upload needed)\n",
    "import os\n",
    "\n",
    "KAGGLE_API_TOKEN = \"KGAT_78cd36a5f2ad172b13fe785d6976fdee\"  # Replace with your token\n",
    "\n",
    "# Set environment variable\n",
    "os.environ['KAGGLE_API_TOKEN'] = KAGGLE_API_TOKEN\n",
    "\n",
    "# Also set username (get it from https://www.kaggle.com/settings)\n",
    "KAGGLE_USERNAME = \"amberfxy\"  # Your Kaggle username\n",
    "os.environ['KAGGLE_USERNAME'] = KAGGLE_USERNAME\n",
    "\n",
    "print(\"✅ Kaggle API token configured!\")\n",
    "print(f\"   Username: {KAGGLE_USERNAME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Download LLM Model\n",
    "\n",
    "Download Mistral 7B GGUF model (~4.1GB).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "import os\n",
    "\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "\n",
    "# Download the dataset\n",
    "!cd data/raw && kaggle datasets download -d aaron7sun/stocknews\n",
    "\n",
    "# Unzip the dataset\n",
    "!cd data/raw && unzip -q stocknews.zip\n",
    "\n",
    "print(\"✅ Dataset downloaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Clone Project Code\n",
    "\n",
    "Clone the project repository from GitHub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Download Mistral 7B model\n",
    "!cd models && wget -q --show-progress https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf\n",
    "\n",
    "print(\"✅ Model downloaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone from GitHub\n",
    "!git clone https://github.com/amberfxy/financial-market-intelligence-rag.git\n",
    "!cp -r financial-market-intelligence-rag/src .\n",
    "!cp -r financial-market-intelligence-rag/ui .\n",
    "!cp -r financial-market-intelligence-rag/scripts .\n",
    "\n",
    "print(\"✅ Code downloaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build FAISS Index\n",
    "\n",
    "Load data, generate embeddings, and build the vector store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "import torch\n",
    "from src.data.loader import load_kaggle_dataset, preprocess_data\n",
    "from src.chunking.chunker import chunk_dataframe\n",
    "from src.embeddings.embedder import BGEEmbedder\n",
    "from src.vectorstore.faiss_store import FAISSStore\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load and preprocess data\n",
    "print(\"Loading dataset...\")\n",
    "df = load_kaggle_dataset('data/raw')\n",
    "df = preprocess_data(df)\n",
    "\n",
    "# Chunk documents\n",
    "print(\"Chunking documents...\")\n",
    "chunks = chunk_dataframe(df, text_column='News Headline', max_tokens=250)\n",
    "\n",
    "# Generate embeddings\n",
    "print(\"Generating embeddings...\")\n",
    "# BGE model will automatically use GPU if available\n",
    "embedder = BGEEmbedder()\n",
    "chunk_texts = [chunk[\"text\"] for chunk in chunks]\n",
    "\n",
    "# Use larger batch size on GPU for faster processing\n",
    "batch_size = 128 if torch.cuda.is_available() else 32\n",
    "print(f\"Using batch size: {batch_size} ({'GPU' if torch.cuda.is_available() else 'CPU'})\")\n",
    "\n",
    "embeddings = embedder.embed_texts(chunk_texts, batch_size=batch_size)\n",
    "\n",
    "# Build FAISS index\n",
    "print(\"Building FAISS index...\")\n",
    "os.makedirs('vectorstore', exist_ok=True)\n",
    "vectorstore = FAISSStore(dimension=embeddings.shape[1])\n",
    "vectorstore.add_chunks(embeddings, chunks)\n",
    "vectorstore.save('vectorstore/faiss.index', 'vectorstore/chunks.pkl')\n",
    "\n",
    "print(f\"✅ Index built successfully! Total chunks: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run Streamlit Application\n",
    "\n",
    "Start the Streamlit web interface for querying the RAG system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Use Colab's built-in port forwarding to access the Streamlit app\n",
    "# Click on the URL shown after running the next cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Streamlit app\n",
    "!streamlit run ui/app.py --server.port=8501 --server.address=0.0.0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- **GPU Runtime**: Enable GPU (Runtime → Change runtime type → GPU) for faster processing\n",
    "- **Data Persistence**: Colab sessions reset when disconnected. Save important files to Google Drive if needed\n",
    "- **Session Timeout**: Colab sessions timeout after 90 minutes of inactivity\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
