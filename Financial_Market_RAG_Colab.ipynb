{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Financial Market Intelligence RAG System\n",
        "## CS6120 Final Project - Colab Version\n",
        "\n",
        "**Team Members:** Soonbee Hwang & Xinyuan Fan (Amber)\n",
        "\n",
        "This notebook runs the complete RAG system in Google Colab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install all required packages\n",
        "!pip install -q torch transformers sentence-transformers faiss-cpu numpy pandas beautifulsoup4 lxml tiktoken streamlit pyngrok\n",
        "\n",
        "# Install llama-cpp-python (CPU version for Colab)\n",
        "!pip install -q llama-cpp-python\n",
        "\n",
        "# Install Kaggle API\n",
        "!pip install -q kaggle\n",
        "\n",
        "print(\"✅ All dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Download Dataset\n",
        "\n",
        "**Option A: Using Kaggle API (Recommended)**\n",
        "\n",
        "1. Go to https://www.kaggle.com/account\n",
        "2. Download your `kaggle.json` API credentials\n",
        "3. Upload it in the cell below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload kaggle.json file\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move kaggle.json to the correct location\n",
        "for fn in uploaded.keys():\n",
        "    if fn == 'kaggle.json':\n",
        "        os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "        !mv kaggle.json /root/.kaggle/\n",
        "        !chmod 600 /root/.kaggle/kaggle.json\n",
        "        print(\"✅ Kaggle credentials configured!\")\n",
        "        break\n",
        "else:\n",
        "    print(\"⚠️ Please upload kaggle.json file\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download dataset\n",
        "import os\n",
        "\n",
        "os.makedirs('data/raw', exist_ok=True)\n",
        "\n",
        "!cd data/raw && kaggle datasets download -d aaron7sun/stocknews\n",
        "!cd data/raw && unzip -q stocknews.zip\n",
        "\n",
        "print(\"✅ Dataset downloaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Download LLM Model\n",
        "\n",
        "Download Mistral 7B GGUF model (~4.1GB)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Download Mistral 7B model\n",
        "!cd models && wget -q --show-progress https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf\n",
        "\n",
        "print(\"✅ Model downloaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Upload Project Code\n",
        "\n",
        "Upload the project source code files, or clone from GitHub:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: Clone from GitHub\n",
        "!git clone https://github.com/amberfxy/financial-market-intelligence-rag.git\n",
        "!cp -r financial-market-intelligence-rag/src .\n",
        "!cp -r financial-market-intelligence-rag/ui .\n",
        "!cp -r financial-market-intelligence-rag/scripts .\n",
        "\n",
        "print(\"✅ Code downloaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Build FAISS Index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "import pandas as pd\n",
        "import logging\n",
        "from src.data.loader import load_kaggle_dataset, preprocess_data\n",
        "from src.chunking.chunker import chunk_dataframe\n",
        "from src.embeddings.embedder import BGEEmbedder\n",
        "from src.vectorstore.faiss_store import FAISSStore\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Load and preprocess data\n",
        "print(\"Loading dataset...\")\n",
        "df = load_kaggle_dataset('data/raw')\n",
        "df = preprocess_data(df)\n",
        "\n",
        "# Chunk documents\n",
        "print(\"Chunking documents...\")\n",
        "chunks = chunk_dataframe(df, text_column='News Headline', max_tokens=250)\n",
        "\n",
        "# Generate embeddings\n",
        "print(\"Generating embeddings...\")\n",
        "embedder = BGEEmbedder()\n",
        "chunk_texts = [chunk[\"text\"] for chunk in chunks]\n",
        "embeddings = embedder.embed_texts(chunk_texts, batch_size=32)\n",
        "\n",
        "# Build FAISS index\n",
        "print(\"Building FAISS index...\")\n",
        "os.makedirs('vectorstore', exist_ok=True)\n",
        "vectorstore = FAISSStore(dimension=embeddings.shape[1])\n",
        "vectorstore.add_chunks(embeddings, chunks)\n",
        "vectorstore.save('vectorstore/faiss.index', 'vectorstore/chunks.pkl')\n",
        "\n",
        "print(f\"✅ Index built successfully! Total chunks: {len(chunks)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup ngrok for public access (optional)\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Get your ngrok authtoken from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "# Uncomment and set your token:\n",
        "# ngrok.set_auth_token(\"YOUR_NGROK_TOKEN\")\n",
        "\n",
        "# Start ngrok tunnel (if using ngrok)\n",
        "# public_url = ngrok.connect(8501)\n",
        "# print(f\"✅ Streamlit app will be available at: {public_url}\")\n",
        "\n",
        "# Alternative: Use Colab's built-in port forwarding\n",
        "print(\"✅ Use Colab's 'Open in new tab' option to access the app\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Streamlit app\n",
        "!streamlit run ui/app.py --server.port=8501 --server.address=0.0.0.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Important Notes\n",
        "\n",
        "1. **Runtime**: Use a GPU runtime (Runtime → Change runtime type → GPU) for faster processing\n",
        "2. **Persistence**: Data will be lost when the session ends. Consider saving to Google Drive\n",
        "3. **Model Size**: The Mistral 7B model is ~4.1GB. Download may take time.\n",
        "4. **Index Building**: Building the FAISS index may take 10-30 minutes depending on data size.\n",
        "5. **Session Timeout**: Colab sessions timeout after inactivity. Keep the tab active.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
